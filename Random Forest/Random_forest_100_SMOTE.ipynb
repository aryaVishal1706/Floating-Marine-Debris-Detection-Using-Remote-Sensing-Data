{"cells":[{"cell_type":"code","execution_count":null,"id":"ae7cd6ac-8db7-4345-8ac0-dcc37d064290","metadata":{"id":"ae7cd6ac-8db7-4345-8ac0-dcc37d064290","outputId":"7386cd8a-83f7-4830-f028-f31fabc9de04"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-05 09:40:46.214428: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-07-05 09:40:50.773320: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-07-05 09:40:57.430084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["# pip install imbalanced-learn\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"id":"d5526130-52c1-4d4c-8d00-e082da14d98d","metadata":{"id":"d5526130-52c1-4d4c-8d00-e082da14d98d","outputId":"89ae301b-44b6-4cd8-f98f-570ceaaca244"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data: X_train shape = (694, 256, 256, 11), y_train shape = (694, 256, 256, 1)\n","Validation data: X_val shape = (328, 256, 256, 11), y_val shape = (328, 256, 256, 1)\n","Testing data: X_test shape = (359, 256, 256, 11), y_test shape = (359, 256, 256, 1)\n"]}],"source":["import os\n","import numpy as np\n","import rasterio\n","from sklearn.impute import SimpleImputer\n","from imblearn.over_sampling import SMOTE\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","def load_dataset(data_dir, img_size=(256, 256)):\n","    \"\"\"\n","    Load dataset from specified directory.\n","\n","    Parameters:\n","    - data_dir (str): Directory containing train, val, and test subdirectories.\n","    - img_size (tuple): Desired size of the image (height, width).\n","\n","    Returns:\n","    - train_data (tuple): Tuple containing (X_train, y_train).\n","    - val_data (tuple): Tuple containing (X_val, y_val).\n","    - test_data (tuple): Tuple containing (X_test, y_test).\n","    \"\"\"\n","    train_dir = os.path.join(data_dir, 'train')\n","    val_dir = os.path.join(data_dir, 'val')\n","    test_dir = os.path.join(data_dir, 'test')\n","\n","    # Load training data\n","    X_train, y_train = load_data_from_dir(os.path.join(train_dir, 'input'), os.path.join(train_dir, 'output'), img_size)\n","    # Load validation data\n","    X_val, y_val = load_data_from_dir(os.path.join(val_dir, 'input'), os.path.join(val_dir, 'output'), img_size)\n","    # Load test data\n","    X_test, y_test = load_data_from_dir(os.path.join(test_dir, 'input'), os.path.join(test_dir, 'output'), img_size)\n","\n","    # Preprocess to handle NaN values\n","    X_train = preprocess_data(X_train)\n","    X_val = preprocess_data(X_val)\n","    X_test = preprocess_data(X_test)\n","\n","    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n","\n","def preprocess_data(images):\n","    \"\"\"\n","    Preprocesses input images to handle NaN values.\n","\n","    Parameters:\n","    - images (numpy.ndarray): Array of input images.\n","\n","    Returns:\n","    - images (numpy.ndarray): Processed array of input images.\n","    \"\"\"\n","    # Replace NaN values with 0\n","    images[np.isnan(images)] = 0\n","\n","    return images\n","\n","def load_data_from_dir(input_dir, output_dir, img_size):\n","    \"\"\"\n","    Load data (images and labels) from input and output directories.\n","\n","    Parameters:\n","    - input_dir (str): Directory containing input images.\n","    - output_dir (str): Directory containing output images.\n","    - img_size (tuple): Desired size of the image (height, width).\n","\n","    Returns:\n","    - images (numpy.ndarray): Array of loaded input images.\n","    - labels (numpy.ndarray): Array of corresponding output images.\n","    \"\"\"\n","    images = []\n","    labels = []\n","\n","    for filename in os.listdir(input_dir):\n","        if filename.endswith('.tif'):\n","            # Load input image (X)\n","            input_path = os.path.join(input_dir, filename)\n","            img = load_tiff_image(input_path, img_size)\n","            images.append(img)\n","\n","            # Load corresponding output image (y)\n","            output_filename = filename.replace('.tif', '_cl.tif')\n","            output_path = os.path.join(output_dir, output_filename)\n","            label = load_tiff_image(output_path, img_size, is_label=True)\n","            label[np.isnan(label)] = 0\n","            labels.append(label)\n","\n","    if images and labels:\n","        images = np.array(images)\n","        labels = np.array(labels)\n","\n","    return images, labels\n","\n","def load_tiff_image(path, img_size, is_label=False):\n","    \"\"\"\n","    Load a TIFF image from specified path.\n","\n","    Parameters:\n","    - path (str): Path to the TIFF image.\n","    - img_size (tuple): Desired size of the image (height, width).\n","    - is_label (bool): Whether the image is a label image.\n","\n","    Returns:\n","    - img (numpy.ndarray): Loaded image as a numpy array.\n","    \"\"\"\n","    with rasterio.open(path) as src:\n","        img = src.read()\n","\n","    # Reshape and resize if necessary\n","    img = img.transpose(1, 2, 0)  # Change from bands x height x width to height x width x bands\n","    img = img[:img_size[0], :img_size[1], :]  # Resize to desired size\n","\n","    if is_label:\n","        # Convert label image to binary classification (1 for Marine Debris, 0 for others)\n","        img = (img == 1).astype(np.uint8)  # Assuming Marine Debris class is encoded as 1\n","\n","    return img\n","\n","# Example usage:\n","data_dir = 'MARIDA'\n","(X_train, y_train), (X_val, y_val), (X_test, y_test) = load_dataset(data_dir)\n","\n","print(f\"Training data: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}\")\n","print(f\"Validation data: X_val shape = {X_val.shape}, y_val shape = {y_val.shape}\")\n","print(f\"Testing data: X_test shape = {X_test.shape}, y_test shape = {y_test.shape}\")\n"]},{"cell_type":"code","execution_count":null,"id":"4a79e500-5738-419b-bea0-ed2fc7eeb6a4","metadata":{"id":"4a79e500-5738-419b-bea0-ed2fc7eeb6a4","outputId":"eec09136-c5c5-4796-ead6-c5b988e1457c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(694, 256, 256, 1)\n"]}],"source":["print(y_train.shape)"]},{"cell_type":"code","execution_count":null,"id":"5691238f-52bd-4a69-8be7-a3e24f18e84d","metadata":{"id":"5691238f-52bd-4a69-8be7-a3e24f18e84d","outputId":"b087febb-3056-44b9-fd13-91cbea689f4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(694, 256, 256, 11)\n"]}],"source":["print(X_train.shape)"]},{"cell_type":"code","execution_count":null,"id":"7afa58f3-b6c6-45fd-8738-376490f28ab0","metadata":{"id":"7afa58f3-b6c6-45fd-8738-376490f28ab0","outputId":"db22a4c4-f764-4932-947e-642bdf77b921"},"outputs":[{"name":"stdout","output_type":"stream","text":["(45481984, 1)\n"]}],"source":["from imblearn.over_sampling import SMOTE\n","X = np.reshape(X_train, (256*256*694, 11))\n","Y = np.reshape(y_train, (256*256*694, 1))\n","print(Y.shape)"]},{"cell_type":"code","execution_count":null,"id":"57efd5ea-7aed-42ba-8684-d3b58b92fb7e","metadata":{"id":"57efd5ea-7aed-42ba-8684-d3b58b92fb7e","outputId":"4a8856df-e1bb-4009-c5f5-1de1524fcc57"},"outputs":[{"name":"stdout","output_type":"stream","text":["(90960082, 11)\n","(90960082,)\n"]}],"source":["sm = SMOTE(random_state=42)\n","X_res, y_res = sm.fit_resample(X, Y)\n","print(X_res.shape)\n","print(y_res.shape)"]},{"cell_type":"code","execution_count":null,"id":"fd1f3c9a-3c1d-45b1-8bba-7505196a7e29","metadata":{"id":"fd1f3c9a-3c1d-45b1-8bba-7505196a7e29","outputId":"cc66c9de-5ed0-4d70-f20b-387d6a0d2be7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00  45480041\n","           1       1.00      1.00      1.00  45480041\n","\n","    accuracy                           1.00  90960082\n","   macro avg       1.00      1.00      1.00  90960082\n","weighted avg       1.00      1.00      1.00  90960082\n","\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# Initialize Random Forest classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","\n","# Train the classifier\n","rf_classifier.fit(X_res, y_res)\n","\n","# Predict on the test set\n","y_pred = rf_classifier.predict(X_res)\n","\n","# Evaluate the model\n","report = classification_report(y_res, y_pred)\n","\n","# Print the classification report\n","print(\"Classification Report:\")\n","print(report)"]},{"cell_type":"code","execution_count":null,"id":"a9d57715-fc10-401d-9bed-ae5351eb9813","metadata":{"id":"a9d57715-fc10-401d-9bed-ae5351eb9813","outputId":"18c05b03-923e-4c58-b37a-ac3b7b2aecff"},"outputs":[{"data":{"text/plain":["['random_forest.joblib']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import joblib\n","\n","joblib.dump(rf_classifier, \"random_forest.joblib\")"]},{"cell_type":"code","execution_count":null,"id":"5ceb296a-05fa-4557-92e3-80fbd35acc09","metadata":{"id":"5ceb296a-05fa-4557-92e3-80fbd35acc09","outputId":"585c455a-993e-40b4-913f-69b3cd33bcf9"},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandomforest.h5\u001b[39m\u001b[38;5;124m'\u001b[39m);\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":[]},{"cell_type":"code","execution_count":null,"id":"b6414b35-3fac-415e-a9d4-0063a3e32f49","metadata":{"id":"b6414b35-3fac-415e-a9d4-0063a3e32f49"},"outputs":[],"source":["\n","# Save the trained model to a file using pickle\n","with open('randomforest_classifier.pkl', 'wb') as f:\n","    pickle.dump(rf_classifier, f)\n","\n","print(\"Model saved successfully as 'randomforest_classifier.pkl'\")"]},{"cell_type":"code","execution_count":null,"id":"27dcef26-cb69-4112-a0e0-881e4c0b0ee5","metadata":{"id":"27dcef26-cb69-4112-a0e0-881e4c0b0ee5"},"outputs":[],"source":["def write_to_file(filename, content):\n","    with open(filename, 'w') as f:\n","        f.write(content)\n","# Example usage:\n","filename = 'output.txt'\n","\n","write_to_file(filename, report)"]},{"cell_type":"code","execution_count":null,"id":"b7a3c6ec-1d1a-4a99-a9f5-ef64d6deea85","metadata":{"id":"b7a3c6ec-1d1a-4a99-a9f5-ef64d6deea85","outputId":"85fba368-107e-4928-caae-7ab458c02f68"},"outputs":[{"data":{"text/plain":["(359, 256, 256, 11)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["X_test.shape"]},{"cell_type":"code","execution_count":null,"id":"d3548a1e-72fc-4c52-99ce-91ce9a2e1f03","metadata":{"id":"d3548a1e-72fc-4c52-99ce-91ce9a2e1f03"},"outputs":[],"source":["test_x = np.reshape(X_test,(359*256*256,11))"]},{"cell_type":"code","execution_count":null,"id":"6caf5990-f136-4584-90ac-4e0725025184","metadata":{"id":"6caf5990-f136-4584-90ac-4e0725025184","outputId":"0dfa7b6c-cac3-4a71-fdb3-1a3439565aa4"},"outputs":[{"data":{"text/plain":["(359, 256, 256, 1)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["y_test.shape"]},{"cell_type":"code","execution_count":null,"id":"5936fdb7-d4cb-4793-85d6-7fe457fdb289","metadata":{"id":"5936fdb7-d4cb-4793-85d6-7fe457fdb289","outputId":"22f687de-291c-47c0-b101-a3eb200c682f"},"outputs":[{"data":{"text/plain":["(23527424, 1)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["test_y = np.reshape(y_test,(359*256*256,1))\n","test_y.shape"]},{"cell_type":"code","execution_count":null,"id":"7d1bd7af-6579-410b-b9ca-5cc4e3778b21","metadata":{"id":"7d1bd7af-6579-410b-b9ca-5cc4e3778b21"},"outputs":[],"source":["result = rf_classifier.predict(test_x)"]},{"cell_type":"code","execution_count":null,"id":"53eeea8f-b8c2-4fa9-b4b9-9c3078a9a770","metadata":{"id":"53eeea8f-b8c2-4fa9-b4b9-9c3078a9a770","outputId":"e18df7e5-75df-4cbf-c692-add7a20bc22a"},"outputs":[{"data":{"text/plain":["'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00  23526792\\n           1       0.39      0.23      0.29       632\\n\\n    accuracy                           1.00  23527424\\n   macro avg       0.69      0.62      0.65  23527424\\nweighted avg       1.00      1.00      1.00  23527424\\n'"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["report = classification_report(result, test_y)\n","report"]},{"cell_type":"code","execution_count":null,"id":"1fabf72f-e2c2-40d3-a9a5-00485824f475","metadata":{"id":"1fabf72f-e2c2-40d3-a9a5-00485824f475","outputId":"6f01c89a-b8bb-4194-f080-2ffb349d6452"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00  23526792\n","           1       0.39      0.23      0.29       632\n","\n","    accuracy                           1.00  23527424\n","   macro avg       0.69      0.62      0.65  23527424\n","weighted avg       1.00      1.00      1.00  23527424\n","\n"]}],"source":["print(report)"]},{"cell_type":"code","execution_count":null,"id":"18056b30-05d3-4cf9-bfdf-8822980478df","metadata":{"id":"18056b30-05d3-4cf9-bfdf-8822980478df"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f707371d-0f06-432b-87fc-f1b6c8a2c5c4","metadata":{"id":"f707371d-0f06-432b-87fc-f1b6c8a2c5c4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"1e157b8f-7903-4e3f-9552-661088dd45be","metadata":{"id":"1e157b8f-7903-4e3f-9552-661088dd45be"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7991b535-c3cf-4d26-8f9b-99244cfabf77","metadata":{"id":"7991b535-c3cf-4d26-8f9b-99244cfabf77"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"8a49731b-620d-479e-9320-2a40b9089d83","metadata":{"id":"8a49731b-620d-479e-9320-2a40b9089d83"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"580757a6-2509-4b96-b5e2-c9f18bda43d6","metadata":{"id":"580757a6-2509-4b96-b5e2-c9f18bda43d6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"c119639c-65f9-4e7a-8bf6-ec2e4e051b16","metadata":{"id":"c119639c-65f9-4e7a-8bf6-ec2e4e051b16"},"outputs":[],"source":["# import os\n","\n","# def shutdown():\n","#     if os.name == 'posix':  # For UNIX/Linux/MacOS\n","#         os.system('shutdown -h now')\n","#     elif os.name == 'nt':  # For Windows\n","#         os.system('shutdown /s /t 1')\n","#     else:\n","#         raise OSError(f\"Unsupported operating system: {os.name}\")\n","\n","# # Calling the shutdown function\n","# shutdown()\n"]},{"cell_type":"code","execution_count":null,"id":"96148f1b-7975-465c-8845-2212da567c90","metadata":{"id":"96148f1b-7975-465c-8845-2212da567c90","outputId":"1d8f6a1a-7895-4100-95ca-f9a075ac3f44"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/isro/.local/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py:619: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (328, 256, 256, 11) (11 channels).\n","  warnings.warn(\n"]}],"source":["# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# # Define ImageDataGenerator with rotation augmentation\n","# datagen = ImageDataGenerator(\n","#     rotation_range=45,  # Rotate images randomly up to 45 degrees\n","#     rescale=1./255  # Normalize pixel values (assuming pixel range 0-255)\n","# )\n","\n","# # Example usage:\n","# batch_size = 32\n","# # Create generators for training and validation data\n","# train_generator = datagen.flow(X_train, y_train, batch_size=batch_size)\n","# val_generator = datagen.flow(X_val, y_val, batch_size=batch_size)\n","\n","# # Note: No need to augment validation data, so we only apply rotation augmentation to training data\n"]},{"cell_type":"code","execution_count":null,"id":"b3fe839d-fd39-4d1b-8ad7-fdd903456095","metadata":{"id":"b3fe839d-fd39-4d1b-8ad7-fdd903456095"},"outputs":[],"source":["# import tensorflow as tf\n","# from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","# # Compute class weights based on frequency\n","# def compute_class_weights(y_train):\n","#     class_weights = {}\n","#     total_samples = len(y_train)\n","#     unique_classes = np.unique(y_train)\n","#     class_counts = np.bincount(y_train.flatten())\n","\n","#     for i, count in enumerate(class_counts):\n","#         class_weights[i] = (1 / count) * (total_samples / len(unique_classes))\n","\n","#     return class_weights\n","\n","# # Example usage\n","# # Assuming y_train is your training labels (shape: (694, 256, 256))\n","# y_train_flat = y_train.flatten()\n","# class_weights = compute_class_weights(y_train_flat)\n","\n","# # Define weighted loss function\n","# loss_function = SparseCategoricalCrossentropy(from_logits=True, weight=class_weights)\n","\n","# # Compile your model with this loss function\n","# model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n","\n","# # Train your model using the generators\n","# history = model.fit(train_generator, epochs=num_epochs, validation_data=val_generator)\n"]},{"cell_type":"code","execution_count":null,"id":"f5d91aa1-3689-4ccf-a9c6-738ce7f0a640","metadata":{"id":"f5d91aa1-3689-4ccf-a9c6-738ce7f0a640"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f4c5c79e-7c2b-4111-9431-9056d50393da","metadata":{"id":"f4c5c79e-7c2b-4111-9431-9056d50393da"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"2ddd0820-77fa-4835-9da0-8115ba3682c7","metadata":{"id":"2ddd0820-77fa-4835-9da0-8115ba3682c7"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}